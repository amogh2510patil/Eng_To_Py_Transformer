{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"english_python_data.txt\", \"r\",encoding=\"utf8\")\n",
    "file_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dps = []\n",
    "dp = None\n",
    "for line in file_lines:\n",
    "  if line[0] == \"#\":\n",
    "    if dp:\n",
    "      dp['solution'] = ''.join(dp['solution'])\n",
    "      dps.append(dp)\n",
    "    dp = {\"question\": None, \"solution\": []}\n",
    "    dp['question'] = line[1:]\n",
    "  else:\n",
    "    dp[\"solution\"].append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4521\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", len(dps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['False', 'None', 'True', '__peg_parser__', 'and', 'as', 'assert', 'async', 'await', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield']\n"
     ]
    }
   ],
   "source": [
    "import keyword\n",
    "\n",
    "print(keyword.kwlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import io\n",
    "from tokenize import tokenize, untokenize\n",
    "def augment_tokenize_python_code(python_code_str, mask_factor=0.3):\n",
    "\n",
    "\n",
    "    var_dict = {} # Dictionary that stores masked variables\n",
    "\n",
    "    # certain reserved words that should not be treated as normal variables and\n",
    "    # hence need to be skipped from our variable mask augmentations\n",
    "    skip_list = ['range', 'enumerate', 'print', 'ord', 'int', 'float', 'zip'\n",
    "                 'char', 'list', 'dict', 'tuple', 'set', 'len', 'sum', 'min', 'max']\n",
    "    skip_list.extend(keyword.kwlist)\n",
    "\n",
    "    var_counter = 1\n",
    "    python_tokens = list(tokenize(io.BytesIO(python_code_str.encode('utf-8')).readline))\n",
    "    tokenized_output = []\n",
    "\n",
    "    for i in range(0, len(python_tokens)):\n",
    "      if python_tokens[i].type == 1 and python_tokens[i].string not in skip_list:\n",
    "\n",
    "        if i>0 and python_tokens[i-1].string in ['def', '.', 'import', 'raise', 'except', 'class']: # avoid masking modules, functions and error literals\n",
    "          skip_list.append(python_tokens[i].string)\n",
    "          tokenized_output.append((python_tokens[i].type, python_tokens[i].string,python_tokens[i].start,python_tokens[i].end,python_tokens[i].line))\n",
    "        elif python_tokens[i].string in var_dict:  # if variable is already masked\n",
    "          tokenized_output.append((python_tokens[i].type, var_dict[python_tokens[i].string],python_tokens[i].start,python_tokens[i].end,python_tokens[i].line))\n",
    "        elif random.uniform(0, 1) > 1-mask_factor: # randomly mask variables\n",
    "          var_dict[python_tokens[i].string] = 'var_' + str(var_counter)\n",
    "          var_counter+=1\n",
    "          tokenized_output.append((python_tokens[i].type, var_dict[python_tokens[i].string],python_tokens[i].start,python_tokens[i].end,python_tokens[i].line))\n",
    "        else:\n",
    "          skip_list.append(python_tokens[i].string)\n",
    "          tokenized_output.append((python_tokens[i].type, python_tokens[i].string,python_tokens[i].start,python_tokens[i].end,python_tokens[i].line))\n",
    "\n",
    "      else:\n",
    "        tokenized_output.append((python_tokens[i].type, python_tokens[i].string,python_tokens[i].start,python_tokens[i].end,python_tokens[i].line))\n",
    "    return untokenize(tokenized_output).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dps2 = []\n",
    "train_expansion_factor = 4\n",
    "for j in range(train_expansion_factor):\n",
    "  for i in range(len(dps)):\n",
    "      try:\n",
    "          dp={}\n",
    "          dp['question']=dps[i]['question']\n",
    "          dp['solution']=augment_tokenize_python_code(dps[i]['solution'])\n",
    "          dps2.append(dp)\n",
    "      except:\n",
    "          pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18084\n"
     ]
    }
   ],
   "source": [
    "print(len(dps2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "# Define a custom pre-tokenizer to handle Python code\n",
    "class PythonPreTokenizer():\n",
    "    def token(self,i, text):\n",
    "        tokenized_output2=[]\n",
    "        row=row_sum=prev_row=0\n",
    "        python_tokens = list(tokenize(io.BytesIO(str(text).encode('utf-8')).readline))\n",
    "        # print(python_tokens)\n",
    "        for i in range(0, len(python_tokens)):        \n",
    "            type_token,token,(srow,scol),(erow,ecol),line=python_tokens[i]\n",
    "            # print(token,srow,scol,erow,ecol)\n",
    "            if srow>row:\n",
    "                row+=1\n",
    "                row_sum+=prev_row\n",
    "            if token:\n",
    "                tokenized_output2.append(text[row_sum+scol:row_sum+ecol])\n",
    "            prev_row=len(line)\n",
    "        return tokenized_output2 if len(tokenized_output2) else [text]\n",
    "    \n",
    "    def pre_tokenize(self, pretok):        \n",
    "        pretok.split(self.token)\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Set the pre-tokenizer to PythonPreTokenizer\n",
    "tokenizer.pre_tokenizer = PreTokenizer.custom(PythonPreTokenizer())\n",
    "\n",
    "# Tokenize a Python code snippet\n",
    "string=\"\"\"def add_two_numbers (num1 ,num2):\n",
    "    sum =num1 +num2 \n",
    "    return sum\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds):\n",
    "    for item in ds:\n",
    "        if item['solution']:\n",
    "            yield item['solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_python_code(python_code_str):\n",
    "#     python_tokens = list(tokenize(io.BytesIO(python_code_str.encode('utf-8')).readline))\n",
    "#     tokenized_output = []\n",
    "#     for i in range(0, len(python_tokens)):\n",
    "#         if python_tokens[i].string=='x':\n",
    "#             tokenized_output.append((python_tokens[i].type, 'var_1',python_tokens[i].start,python_tokens[i].end,python_tokens[i].line))\n",
    "#             continue                        \n",
    "#         tokenized_output.append((python_tokens[i].type, python_tokens[i].string,python_tokens[i].start,python_tokens[i].end,python_tokens[i].line))\n",
    "#     return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string2=dps[1736]['solution']\n",
    "# python_tokens = list(tokenize(io.BytesIO(string2.encode('utf-8')).readline))\n",
    "# # python_tokens[3]='var1'\n",
    "# python_tokens2=tokenize_python_code(string2)\n",
    "# print(untokenize(python_tokens2).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(augment_tokenize_python_code(string2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dps2[1736]['solution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_tokenize_python_code(dps2[1736]['solution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n', (1, 2)), ('def', (2, 5)), ('isBuzz', (6, 12)), ('(', (12, 13)), ('num', (13, 16)), (')', (16, 17)), (':', (18, 19)), ('\\n', (20, 21)), ('\\n', (23, 24)), ('    ', (24, 28)), ('return', (28, 34)), ('(', (35, 36)), ('num', (36, 39)), ('%', (40, 41)), ('10', (42, 44)), ('==', (45, 47)), ('7', (48, 49)), ('or', (50, 52)), ('num', (53, 56)), ('%', (57, 58)), ('7', (59, 60)), ('==', (61, 63)), ('0', (64, 65)), (')', (65, 66)), ('\\n', (68, 69)), ('\\n', (71, 72))]\n"
     ]
    }
   ],
   "source": [
    "for x in get_all_sentences(dps[3662:3663]):\n",
    "    print(tokenizer.pre_tokenizer.pre_tokenize_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', (0, 1)), ('=', (2, 3)), ('var_1', (3, 8)), ('<<', (9, 11)), ('2', (11, 12)), ('\\n', (13, 14)), ('print', (14, 19)), ('(', (20, 21)), ('\"Binary Left Shift\"', (21, 40)), (',', (40, 41)), ('c', (41, 42)), (')', (43, 44))]\n"
     ]
    }
   ],
   "source": [
    "s=\"\"\"c =var_1 <<2 \n",
    "print (\"Binary Left Shift\",c )\"\"\"\n",
    "print(tokenizer.pre_tokenizer.pre_tokenize_str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"])\n",
    "tokenizer.train_from_iterator(get_all_sentences(dps2), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6760"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('tokenizer_solution.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=Tokenizer.from_file('tokenizer_solution.json')\n",
    "\n",
    "tok.pre_tokenizer = PreTokenizer.custom(PythonPreTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 1384, 5, 92, 7, 104, 6, 8, 4, 16, 68, 9, 92, 26, 104, 4, 21, 68]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.encode(string).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def add_two_numbers ( num1 , num2 ) : \n",
      "      sum = num1 + num2 \n",
      " return sum\n"
     ]
    }
   ],
   "source": [
    "print(tok.decode([18, 1390, 5, 102, 7, 111, 6, 8, 4, 16, 68, 9, 102, 26, 111, 4, 21, 68]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 1384, 5, 92, 7, 104, 6, 8, 4, 16, 68, 9, 92, 26, 104, 4, 21, 68]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(string).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def add_two_numbers ( num1 , num2 ) : \n",
      "      sum = num1 + num2 \n",
      " return sum\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([18, 1384, 5, 92, 7, 104, 6, 8, 4, 16, 68, 9, 92, 26, 104, 4, 21, 68]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import tokenize, untokenize\n",
    "import io\n",
    "\n",
    "def tokenize_python_code(python_code_str):\n",
    "    python_tokens = list(tokenize(io.BytesIO(python_code_str.encode('utf-8')).readline))\n",
    "    tokenized_output=[]\n",
    "    tokenized_output2=[]\n",
    "    row=0\n",
    "    row_sum=0\n",
    "    prev_row=0\n",
    "    for i in range(0, len(python_tokens)):\n",
    "        tokenized_output.append(python_tokens[i].string)\n",
    "        type,token,(srow,scol),(erow,ecol),line=python_tokens[i]\n",
    "        # print(type,token,srow,scol,erow,ecol)\n",
    "        if srow>row:\n",
    "            row+=1\n",
    "            row_sum+=prev_row\n",
    "        tokenized_output2.append(python_code_str[row_sum+scol:row_sum+ecol])\n",
    "        prev_row=len(line)\n",
    "    print(tokenized_output)\n",
    "    print(tokenized_output2)\n",
    "    return tokenized_output\n",
    "\n",
    "# def handle_token(type,token,(srow,scol),(erow,ecol),line):\n",
    "#     print(srow,scol,erow,ecol,tokenize.tok_name[type],repr(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string[28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string=\"\"\"def add_two_numbers (num1 ,num2):\n",
    "    sum =num1 +num2 \n",
    "    return sum\"\"\"\n",
    "\n",
    "\n",
    "toks=tokenize_python_code(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from train import dataset_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import get_or_build_tokenizer\n",
    "from config import get_config\n",
    "config=get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_src = get_or_build_tokenizer(config, data, 'question')\n",
    "tokenizer_tgt = get_or_build_tokenizer(config, data, 'solution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97, 9, 636, 4, 107, 9, 688, 4, 68, 9, 97, 26, 107, 4, 13, 5, 1197, 6, 4, 4, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_tgt.encode(data[0]['solution']).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_1 = IplData \n",
      " join = 'z' \n",
      " sum = var_1 + join \n",
      " print ( longest_non_repeat ) \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_tgt.decode([10, 9, 639, 4, 111, 9, 686, 4, 68, 9, 10, 26, 111, 4, 13, 5, 1215, 6, 4, 4, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import BilingualDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BilingualDataset(data, tokenizer_src, tokenizer_tgt, 'question', 'solution', config['seq_len'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 81\n",
      "Max length of target sentence: 387\n"
     ]
    }
   ],
   "source": [
    "max_len_src = 0\n",
    "max_len_tgt = 0\n",
    "lst=[]\n",
    "\n",
    "for item in data:\n",
    "    src_ids = tokenizer_src.encode(item['question']).ids\n",
    "    tgt_ids = tokenizer_tgt.encode(item['solution']).ids\n",
    "    max_len_src = max(max_len_src, len(src_ids))\n",
    "    max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "    lst.append(len(tgt_ids))\n",
    "\n",
    "print(f'Max length of source sentence: {max_len_src}')\n",
    "print(f'Max length of target sentence: {max_len_tgt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294639616"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0).total_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(1, device=\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.cuda.memory_reserved(0)\n",
    "av = torch.cuda.memory_allocated(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097152"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
